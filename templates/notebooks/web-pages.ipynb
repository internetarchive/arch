{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srouuJ3tCYqf"
   },
   "source": [
    "# Web Pages Dataset Exploration\n",
    "\n",
    "We're going to take a look at a few examples of how we can explore the Web Pages dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRYzc-YZ8BQ_",
    "outputId": "fc2c6b3b-acb3-4aea-a23c-482e4799c3a7"
   },
   "outputs": [],
   "source": [
    "dataset = \"ARCHDATASETURL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z14F2cIWJVW0"
   },
   "source": [
    "## pandas\n",
    "\n",
    "Next, we'll setup our environment so we can load our Web Pages Information dataset into [pandas](https://pandas.pydata.org) DataFrames. If you're unfamiliar with DataFrames, but you've worked with spreadsheets before, you should feel comfortable pretty quick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS0T6i4xPTVx"
   },
   "source": [
    "# Environment\n",
    "\n",
    "Next, we'll setup our environment so we can load our derivatives into [pandas](https://pandas.pydata.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n56-DeEFPTCq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgBtx0xFFv_i"
   },
   "source": [
    "# Data Table Display\n",
    "\n",
    "Colab includes an extension that renders pandas DataFrames into interactive displays that can be filtered, sorted, and explored dynamically. This can be very useful for taking a look at what each DataFrame provides!\n",
    "\n",
    "Data table display for pandas DataFrames can be enabled by running:\n",
    "```python\n",
    "%load_ext google.colab.data_table\n",
    "```\n",
    "and disabled by running\n",
    "```python\n",
    "%unload_ext google.colab.data_table\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8h2lJY2Z7xyt"
   },
   "outputs": [],
   "source": [
    "%load_ext google.colab.data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6prR7j1zI_D5"
   },
   "source": [
    "## Loading our ARCH Dataset as a DataFrame\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Next, we'll create pandas DataFrame from our dataset, and show a preview of it using the Data Table Display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "_VUJrBueC8hy",
    "outputId": "9480545e-b796-4b19-96dd-ef637fc8e219"
   },
   "outputs": [],
   "source": [
    "web_pages = pd.read_csv(dataset, compression=\"gzip\", skipinitialspace=True)\n",
    "web_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTcTLMHmTGXo"
   },
   "source": [
    "## Text Analysis\n",
    "\n",
    "Next, we'll do some basic text analysis with our `web_pages` DataFrame with `nltk` and`spaCy`, and end with a word cloud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPzVa3WhTImQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QkkITU2TkkB",
    "outputId": "3752a3a1-99d6-4ef1-c8da-5a7c88fb957e"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x1k3fmHu1NU"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvmymu758duP"
   },
   "source": [
    "We'll drop the `NaN` values in our DataFrame to clean things up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "nBEVNyMxjtHg",
    "outputId": "c2eac235-b74b-42f6-fca0-5b72827492c8"
   },
   "outputs": [],
   "source": [
    "web_pages = web_pages.dropna()\n",
    "web_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dArfB41Vp5Rx"
   },
   "source": [
    "We need to set the [`mode.chained_assignment`](https://pandas.pydata.org/docs/user_guide/options.html?highlight=chained_assignment) to `None` now to silence some exception errors that will come up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UelY00RWxs_-"
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCg-yGKl8ne8"
   },
   "source": [
    "Next, we'll setup a tokenizer which will split on words, and create a new column which is the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwtFa9FfzLjC"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGSTGsdHr0l3"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokenized\"] = web_pages[\"content\"].map(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e4qcA8p85Od"
   },
   "source": [
    "Now well create a column with the tokenized value count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOXKqykU0Vgc"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokens\"] = web_pages[\"content_tokenized\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1LvlTd-9GC9"
   },
   "source": [
    "### Basic word count statistics with pandas!\n",
    "\n",
    "Now we can use the power of pandas [Statisitcal functions](https://pandas.pydata.org/docs/user_guide/computation.html) to show us some basic statistics about the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wvUcbx29M4n"
   },
   "source": [
    "**Mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwO0KHnS9JJQ",
    "outputId": "e56301fa-9fd9-4784-f56e-644aec235cbd"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokens\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idMyBDrU9V3O"
   },
   "source": [
    "**Standard deviation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-V6zi079WNv",
    "outputId": "2cec996c-a053-408d-cfe0-a03b3d94d8b5"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokens\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai0QVx9t9bnt"
   },
   "source": [
    "**Max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbXjWwW99bzr",
    "outputId": "2db38580-6cd0-4d3d-8982-ee58f50be331"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokens\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLT5HLXM9f8W"
   },
   "source": [
    "**Min**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "629g31k39gH2",
    "outputId": "d2389d4c-47c6-499d-cc66-70edf311e681"
   },
   "outputs": [],
   "source": [
    "web_pages[\"content_tokens\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-bIw1Pu2NkW"
   },
   "source": [
    "### Pages with most words\n",
    "\n",
    "Let's create a bar chart that shows the pages with the most words. Here we can see the power of pandas at work, in terms of both analysis and visualization.\n",
    "\n",
    "First, let's show the query to get the data for our chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oktgvxIr2cSV"
   },
   "outputs": [],
   "source": [
    "word_count = (\n",
    "    web_pages[[\"url\", \"content_tokens\"]]\n",
    "    .sort_values(by=\"content_tokens\", ascending=False)\n",
    "    .head(25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "3Eeserah2lBX",
    "outputId": "9188b488-729a-4f5f-8af4-be0d36af2353"
   },
   "outputs": [],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uepwHXS59tz-"
   },
   "source": [
    "Next, let's create a bar chart of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "ns1ZlwbnqoDi",
    "outputId": "5391c29c-aee5-4f9f-bf72-7c75f12e8522"
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "word_count_bar = (\n",
    "    alt.Chart(word_count)\n",
    "    .mark_bar()\n",
    "    .encode(x=alt.X(\"url:O\", sort=\"-y\"), y=alt.Y(\"content_tokens:Q\"))\n",
    ")\n",
    "\n",
    "word_count_rule = (\n",
    "    alt.Chart(word_count).mark_rule(color=\"red\").encode(y=\"mean(content_tokens):Q\")\n",
    ")\n",
    "\n",
    "word_count_text = word_count_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(\n",
    "    text=\"content_tokens:Q\"\n",
    ")\n",
    "\n",
    "(word_count_bar + word_count_rule + word_count_text).properties(\n",
    "    width=1400, height=700, title=\"Pages with the most words\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhLIldv-3SOm"
   },
   "source": [
    "### How about NER on the page with the most tokens?\n",
    "\n",
    "[Named-Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition), or NER, is an exciting field of natural language processing that lets us extract \"entities\" out of text; the names of people, locations, or organizations.\n",
    "\n",
    "To do this, we first need to find the pages that have the most tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Svg3ruY3cQg",
    "outputId": "111faf9d-b82e-4eef-9c5e-e3e75428ab29"
   },
   "outputs": [],
   "source": [
    "word_count_max = (\n",
    "    web_pages[[\"url\", \"content_tokens\", \"content\"]]\n",
    "    .sort_values(by=\"content_tokens\", ascending=False)\n",
    "    .head(1)\n",
    ")\n",
    "word_count_max[\"url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvufosIP-7JH"
   },
   "source": [
    "We'll remove the column width limit so we can check out our content for the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gLaHabY-vRM"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBxWH3MV_Gp-"
   },
   "source": [
    "Let's take a look at our page's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "5WykQM1H3YIU",
    "outputId": "6ddbef91-ea46-4f51-f0e9-3209728bd7bd"
   },
   "outputs": [],
   "source": [
    "page = word_count_max[\"content\"].astype(\"unicode\").to_string()\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVEwDKpR5QcB"
   },
   "source": [
    "\n",
    "#### Setup spaCy\n",
    "\n",
    "We now need to set up [spaCy](https://en.wikipedia.org/wiki/SpaCy), a natural-language processing toolkit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D8tBLZz3wBp"
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.max_length = 1100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIPHPYyIEEwN"
   },
   "source": [
    "Next we'll run the natual language processor from SpaCy, and then display the NER output. Watch how it finds organizations, people, and beyond!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7LNVyHX633qT",
    "outputId": "603dce93-96e3-4af0-fe18-553505116eb0"
   },
   "outputs": [],
   "source": [
    "ner = nlp(page)\n",
    "displacy.render(ner, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtGdA7jjkmHg"
   },
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH3vFgIHk2tB"
   },
   "source": [
    "We'll be using the [vaderSentiment](https://github.com/cjhutto/vaderSentiment) library and [adapting examples](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/04-Sentiment-Analysis.html#) from Melanie Walsh's [\"Introduction to Cultural Analytics & Python\"](https://melaniewalsh.github.io/Intro-Cultural-Analytics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFx6dex-kuiY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LTJDMRUlIP6"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER\n",
    "sentimentAnalyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvST6DPxmxbI"
   },
   "source": [
    "We'll create a function, that we'll then apply to a DataFrame to create sentiment analysis scores for the `content` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xs8w1J5GmDEQ"
   },
   "outputs": [],
   "source": [
    "def calculate_sentiment(text):\n",
    "    # Run VADER on the text\n",
    "    scores = sentimentAnalyser.polarity_scores(text)\n",
    "    # Extract the compound score\n",
    "    compound_score = scores[\"compound\"]\n",
    "    # Return compound score\n",
    "    return compound_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g33U8Dflp9an"
   },
   "source": [
    "Since it will take some time to run the sentiment analysis on the entire `web_pages` DataFrame, we'll create a sample from `web_pages`, and run the sentiment analysis on that for demostration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zCsMvUdqRXy"
   },
   "outputs": [],
   "source": [
    "web_pages_sample = web_pages.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFKu-qksl4kC"
   },
   "outputs": [],
   "source": [
    "web_pages_sample[\"sentiment_score\"] = web_pages_sample[\"content\"].apply(\n",
    "    calculate_sentiment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1XrTYhRrEqN"
   },
   "source": [
    "Let's see what the the scores look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WVZXHG0Nq_TS",
    "outputId": "5754c9bd-bf09-4d6a-deda-fd809ee7468b"
   },
   "outputs": [],
   "source": [
    "web_pages_sample[[\"sentiment_score\", \"content\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uziox47r9Ld"
   },
   "source": [
    "Finally, let's plot the sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "id": "d2wLS4KEriy_",
    "outputId": "b1cf4ed1-332c-41f3-9f11-2733b50c8995"
   },
   "outputs": [],
   "source": [
    "sentiment_scores = (\n",
    "    web_pages_sample[[\"sentiment_score\"]].value_counts().head(10).reset_index()\n",
    ")\n",
    "sentiment_scores = sentiment_scores.rename(\n",
    "    {\"sentiment_score\": \"Sentiment Score\", 0: \"Count\"}, axis=1\n",
    ")\n",
    "\n",
    "sentiment_chart = (\n",
    "    alt.Chart(sentiment_scores)\n",
    "    .mark_circle()\n",
    "    .encode(\n",
    "        x=alt.X(\"Sentiment Score:Q\", bin=True),\n",
    "        y=alt.Y(\"Count:Q\", bin=True),\n",
    "        size=\"Count\",\n",
    "    )\n",
    ")\n",
    "\n",
    "sentiment_chart.properties(width=1400, height=700, title=\"Sentiment Score Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-KLFb415e1y"
   },
   "source": [
    "### Wordcloud\n",
    "\n",
    "What better way to wrap-up this notebook than by creating a word cloud!\n",
    "\n",
    "Word clouds are always fun, right?! They're an interesting way to visualize word frequency, as the more times that a word occurs, the larger it will appear in the word cloud.\n",
    "\n",
    "Let's setup some dependencies here. We will install the [word_cloud](https://github.com/amueller/word_cloud) library and setup some stop words via `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiS7nhq2eirZ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install wordcloud\n",
    "from wordcloud import ImageColorGenerator, WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvKSPMvE95m_"
   },
   "source": [
    "Let's remove the remove the stopwords from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0E06S_xO5-7c"
   },
   "outputs": [],
   "source": [
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbcXX78s6pFH"
   },
   "outputs": [],
   "source": [
    "web_pages[\"stopwords\"] = web_pages[\"content_tokenized\"].apply(\n",
    "    lambda x: [item.lower() for item in x if item not in stopwords]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6IrEAcTEorT"
   },
   "source": [
    "Next we'll pull 500 rows of values from our new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-DROr3veo3J"
   },
   "outputs": [],
   "source": [
    "words = web_pages[\"stopwords\"].head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgiOIFUU9_3I"
   },
   "source": [
    "Now we can create a word cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "2Kp0xnU1eud6",
    "outputId": "0cef1945-52bd-41c5-d484-4ccba43080b8"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=2000,\n",
    "    height=1500,\n",
    "    scale=10,\n",
    "    max_font_size=250,\n",
    "    max_words=100,\n",
    "    background_color=\"white\",\n",
    ").generate(str(words))\n",
    "plt.figure(figsize=[35, 10])\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
